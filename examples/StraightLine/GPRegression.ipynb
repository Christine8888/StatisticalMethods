{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "In this example, we return to our \"straight line\" mock dataset, and investigate a \"model-free model\" for it: a Gaussian Process. The idea is to find a flexible model that can _interpolate_ between the data we have, in order to predict future data lying in the gaps, or beyond the observed domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "* Let's generate a simple Cepheids-like dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values.\n",
    "\n",
    "\n",
    "> You can make a different dataset by choosing a different random, to see how the conclusions change from dataset to datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from straightline_utils import *\n",
    "\n",
    "(x, y, sigmay) = generate_data(seed=13)\n",
    "\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Gaussian Process\n",
    "\n",
    "Let's follow [Jake VanderPlas' `astroML` example](http://www.astroml.org/book_figures/chapter8/fig_gp_example.html#book-fig-chapter8-fig-gp-example), to see how to work with the `scikit-learn` v0.17 Gaussian Process model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining a GP\n",
    "\n",
    "First we define a kernel function, for populating the covariance matrix of our GP. To avoid confusion, a Gaussian kernel is referred to as a \"squared exponential\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_squared_exponential(x1, x2, theta0):\n",
    "    return np.exp(-theta0 * (x1 - x2) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's draw some samples from the unconstrained process. Each sample is a function $y(x)$, which we evaluate on a grid. We'll need to assert a value for the kernel hyperparameter $h$, which dictates the correlation length between the datapoints. That will allow us to compute a mean function (which for simplicity we'll set to the mean observed $y$ value), and a covariance matrix that captures the correlations between datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xgrid = np.linspace(0, 350, 100)\n",
    "print(\"y(x) will be predicted on a grid of length\", len(xgrid))\n",
    "\n",
    "h = 10.0\n",
    "theta0 = 0.5 / h**2\n",
    "print(\"Correlation parameters h, theta0: \",h, theta0)\n",
    "\n",
    "# Set up the mean function and covariance matrix. Note the broadcasting to a 2D array, \n",
    "# achieved by passing in x[:, None] which has shape (100, 1).\n",
    "\n",
    "mu = np.zeros(len(xgrid))\n",
    "C = my_squared_exponential(xgrid, xgrid[:, None], theta0)\n",
    "\n",
    "print(\"Set up covariance matrix C with shape \", C.shape)\n",
    "\n",
    "# Draw three sample y(x) functions:\n",
    "draws = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "print(\"Drew 3 samples, stored in an array with shape \", draws.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot these, to see what our prior looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start a 4-panel figure:\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot our three prior draws:\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(xgrid, draws[0].T, '-r')\n",
    "ax.plot(xgrid, draws[1].T, '-g')\n",
    "ax.plot(xgrid, draws[2].T, '-b', label='Prior $y(x)$')\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y(x)$')\n",
    "ax.legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each predicted $y(x)$ is drawn from a Gaussian of unit variance, and with off-diagonal elements determined by the covariance function. Try changing `h` to see what happens to the smoothness of the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our data to be well interpolated by this Gaussian Process, it will need to be rescaled such that it has zero mean and unit variance. There are [standard methods for doing this](http://scikit-learn.org/stable/modules/preprocessing.html), but we'll do this rescaling here for transparency - and so we know what to add back in later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Rescaler():\n",
    "    def __init__(self, y, err):\n",
    "        self.original_data = y\n",
    "        self.original_err = err\n",
    "        self.mean = np.mean(y)\n",
    "        self.std = np.std(y)\n",
    "        self.transform()\n",
    "        return\n",
    "    def transform(self):\n",
    "        self.y = (self.original_data - self.mean) / self.std\n",
    "        self.err = self.original_err / self.std\n",
    "        return()\n",
    "    def invert(self, scaled_y, scaled_err):\n",
    "        return (scaled_y * self.std + self.mean, scaled_err * self.std)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rescaled = Rescaler(y, sigmay)\n",
    "print('Mean, variance of rescaled data: ',np.round(np.mean(rescaled.y)), np.round(np.var(rescaled.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we can undo the scaling, for any `y` and `sigmay`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2, sigmay2 = rescaled.invert(rescaled.y, rescaled.err)\n",
    "print('Maximum differences in y, sigmay, after round trip: ',np.max(np.abs(y2 - y)), np.max(np.abs(sigmay2 - sigmay)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constraining the GP\n",
    "\n",
    "Now, using the same covariance function, lets \"fit\" the GP by constraining each draw from the GP to go through our data points. Let's first look at how this would work for two data points with no uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x1 = np.array([x[10], x[12]])\n",
    "rescaled_y1 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay1 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model:\n",
    "gp1 = GaussianProcess(corr='squared_exponential', theta0=theta0,\n",
    "                      thetaL=0.0001, thetaU=1000.0,\n",
    "                      random_state=0)\n",
    "\n",
    "# Fit it to our two noiseless datapoints:\n",
    "gp1.fit(x1[:, None], rescaled_y1)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid1, MSE1 = gp1.predict(xgrid[:, None], eval_MSE=True)\n",
    "rescaled_ygrid1_err = np.sqrt(MSE1)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid1, ygrid1_err = rescaled.invert(rescaled_ygrid1, rescaled_ygrid1_err)\n",
    "y1, sigmay1 = rescaled.invert(rescaled_y1, rescaled_sigmay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(222)\n",
    "ax.plot(xgrid, ygrid1, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill_between(xgrid, ygrid1 - ygrid1_err, ygrid1 + ygrid1_err, color='gray', alpha=0.3)\n",
    "ax.plot(x1, y1, '.k', ms=6, label='Noiseless constraints')\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(0, 250)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the absence of information, the GP defaults to its mean function, which we chose to be a constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Including Observational Uncertainties\n",
    "\n",
    "The mechanism for including uncertainties is a little esoteric: `scikit-learn` wants to be given a \"nugget\" to multiply the diagonal elements of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x2 = np.array([x[10], x[12]])\n",
    "rescaled_y2 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay2 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, including a nugget of observational errors:\n",
    "gp2 = GaussianProcess(corr='squared_exponential', theta0=theta0,\n",
    "                      thetaL=0.0001, thetaU=1000.0,\n",
    "                      nugget=(rescaled_sigmay2 / rescaled_y2) ** 2, \n",
    "                      random_state=0)\n",
    "\n",
    "# Fit it to our two noisy datapoints:\n",
    "gp2.fit(x2[:, None], rescaled_y2)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid2, MSE2 = gp2.predict(xgrid[:, None], eval_MSE=True)\n",
    "rescaled_ygrid2_err = np.sqrt(MSE2)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid2, ygrid2_err = rescaled.invert(rescaled_ygrid2, rescaled_ygrid2_err)\n",
    "y2, sigmay2 = rescaled.invert(rescaled_y2, rescaled_sigmay2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(223)\n",
    "ax.plot(xgrid, ygrid2, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill_between(xgrid, ygrid2 - ygrid2_err, ygrid2 + ygrid2_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x2, y2, sigmay2, fmt='.k', ms=6, label='Noisy constraints')\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(0, 250)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y(x)$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using all the Data\n",
    "\n",
    "Now let's extend the above example to use all of our datapoints. This additional information should pull the predictions further away from the initial mean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use all of our datapoints:\n",
    "x3 = x\n",
    "rescaled_y3 = rescaled.y\n",
    "rescaled_sigmay3 = rescaled.err\n",
    "\n",
    "# Instantiate a GP model, including a nugget of observational errors:\n",
    "gp3 = GaussianProcess(corr='squared_exponential', theta0=theta0,\n",
    "                      thetaL=0.0001, thetaU=1000.0,\n",
    "                      nugget=(rescaled_sigmay3 / rescaled_y3) ** 2,\n",
    "                      random_state=0)\n",
    "\n",
    "# Fit it to our noisy datapoints:\n",
    "gp3.fit(x3[:, None], rescaled_y3)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid3, MSE3 = gp3.predict(xgrid[:, None], eval_MSE=True)\n",
    "rescaled_ygrid3_err = np.sqrt(MSE3)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid3, ygrid3_err = rescaled.invert(rescaled_ygrid3, rescaled_ygrid3_err)\n",
    "y3, sigmay3 = rescaled.invert(rescaled_y3, rescaled_sigmay3)\n",
    "\n",
    "# We have fit for the `h` parameter: print the result here:\n",
    "print(\"Best-fit h =\", np.sqrt(0.5 / gp3.theta_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(224)\n",
    "ax.plot(xgrid, ygrid3, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill_between(xgrid, ygrid3 - ygrid3_err, ygrid3 + ygrid3_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x3, y3, sigmay3, fmt='.k', ms=6, label='Data')\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(0, 250)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now see the Gaussian Process model providing a smooth interpolation between the points. Show here is the posterior mean curve; samples drawn from the GP will show fluctuations, but all will be plausible under our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnote\n",
    "\n",
    "In `scikit-learn` v0.18, the `GaussianProcessRegressor` model provides the ability to draw samples from the posterior PDF for $P(y(x))$, and a more intuitive API for constructing kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kernel = RBF(length_scale=10, length_scale_bounds=(0.01, 100.0))\n",
    "# gp0 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

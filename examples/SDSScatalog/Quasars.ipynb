{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in the SDSS catalog: \n",
    "## Photometric Redshift Estimation and Quasar Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern wide field surveys are generating very large databases of automatically measured objects, whose error properties may not be well understood. Fast machine learning algorithms have been proved to be very useful in such a regime.\n",
    "\n",
    "Let's investigate the SDSS photometric object catalog, and look for machine learning solutions to the following two problems:\n",
    "\n",
    "1. Estimating the redshifts of quasars from their photometry (regression)\n",
    "\n",
    "1. Selecting quasars from a background of stars and galaxies (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\">\n",
    " <pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE \n",
    " (class = 'QSO')\n",
    " ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "We've got 1000 stars and 1000 galaxies as well, and saved them for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For pretty plotting\n",
    "!pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photometric Redshift Estimation\n",
    "\n",
    "* This is a regression problem, to be able to predict the redshift response variable given a number of photometric measurement \"features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qsos = pd.read_csv(\"data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins =  hist(qso_redshifts.values,bins=100) ; xlabel(\"redshift\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty clearly a big cut at around $z=2$.\n",
    "\n",
    "Let's plot all the features, colored by the target redshift, to look for structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(qso_features[0:2000],alpha=0.2,figsize=[15,15],color=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = qso_features.values  # 9-d feature space\n",
    "y = qso_redshifts.values # redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do?\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "print (\"Linear regression: MSE = \",mse)\n",
    "print(\"R2 score =\",linear.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test,y_lr_pred - y_test,'o',alpha=0.2)\n",
    "title(\"Linear Regression Residuals - MSE = %.1f\" % mse)\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Residual\")\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *k*-Nearest Neighbor (KNN) Regression **\n",
    "\n",
    "[\"Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X) # Many methods work better on scaled X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "KNN.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_knn_pred)\n",
    "print(\"MSE (KNN) =\", mse)\n",
    "print(\"R2 score (KNN) =\",KNN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_knn_pred - y_test,'o',alpha=0.2)\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse)\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Residual\")\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's do a grid search over the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "> KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
    "\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# And we'll need a cv iterator:\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(len(X), 10, test_size=0.4)\n",
    "\n",
    "# Compute our cv scores for a range of the no. of neighbors:\n",
    "from sklearn.learning_curve import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "Can you explain the shapes of these two curves? Talk to your neighbor for a few minutes, and be prepared to suggest reasons for a) the rise and fall of the cross validation score and b) the monotonic decrease in training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "all_the_KNNs = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `GridSearchCV` object behaves just like a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_the_KNNs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_the_KNNs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_the_KNNs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../scikit-learn/figures/grid_search_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT: what we want is a cross-validated model - so we need to pass our `GridSearch` to the cross validation score calculator to see the best we can do.\n",
    "\n",
    "This will take a few moments, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(all_the_KNNs, X_scaled, y, cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanR2,errR2 = np.mean(R2),np.std(R2)\n",
    "print(\"Mean score:\",meanR2,\"+/-\",errR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(all_the_KNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN = neighbors.KNeighborsRegressor(n_neighbors=16,p=1,weights='distance')\n",
    "KNN.fit(X_train,y_train)\n",
    "y_knn_pred = KNN.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_knn_pred)\n",
    "print(\"MSE (KNN) =\", mse)\n",
    "print(\"R2 score (KNN) =\",KNN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_knn_pred - y_test,'o',alpha=0.2)\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse)\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Residual\")\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasar Classification with Random Forests\n",
    "\n",
    "\n",
    "* Let's do a 3-class classification problem: star, galaxy, or QSO.\n",
    "\n",
    "\n",
    "* A very good general-purpose classification (and regression!) algorithm is Random Forest. See [this blog post](http://blog.yhathq.com/posts/random-forests-in-python.html) for a nice high level introduction.\n",
    "\n",
    "\n",
    "* [\"A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset, and uses averaging to improve the predictive accuracy and control over-fitting.](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "* Let's read in equal numbers of all three types of data, clean them up, and set $y$ equal to the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sources = pd.read_csv(\"data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"])[:1000]\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/star1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/galaxy1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources[(all_sources[\"dered_r\"] > -9999) & (all_sources[\"g_r_color\"] > -10) & (all_sources[\"g_r_color\"] < 10)]\n",
    "\n",
    "all_labels = all_sources[\"class\"]\n",
    "\n",
    "all_features = copy.copy(all_sources)\n",
    "del all_features[\"class\"]\n",
    "\n",
    "X = copy.copy(all_features.values)\n",
    "y = copy.copy(all_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape =\", X.shape)\n",
    "print(\"Class label vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be more convenient to have our class labels be integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y[y==\"QSO\"] = 0\n",
    "# y[y==\"STAR\"] = 1\n",
    "# y[y==\"GALAXY\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - let's turn on the machine learning:\n",
    "\n",
    "** Random Forest Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200,oob_score=True)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the important features in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(all_sources.columns.values,clf.feature_importances_),key=lambda q: q[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.oob_score_  ## \"Out of Bag\" Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier improvement with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a classifier:\n",
    "sdss_rf = RandomForestClassifier(n_estimators=200,oob_score=True)\n",
    "\n",
    "# Parameter to search:\n",
    "parameters = {'n_estimators':(10,50,200),\"max_features\": [\"auto\",3,5],\n",
    "              'criterion':[\"gini\",\"entropy\"],\"min_samples_leaf\": [1,2]}\n",
    "\n",
    "# Initial training/test split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a grid search to find the highest 3-fold CV score:\n",
    "rf_tune = GridSearchCV(sdss_rf, parameters, cv=3, verbose=1)\n",
    "rf_opt = rf_tune.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and estimator\n",
    "print(rf_opt.best_score_)\n",
    "print(rf_opt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_opt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = rf_tune.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

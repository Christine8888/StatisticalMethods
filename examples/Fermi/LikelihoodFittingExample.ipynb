{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import LikeFitUtils as lfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, read in the input data\n",
    "inputDict = lfu.LoadFromFitsFile(\"data/draco_srcTemplates.fits\")\n",
    "\n",
    "# Parse out the bits we need\n",
    "n_obs = inputDict[\"DATA\"]  # This is the actual data\n",
    "fixed = inputDict[\"FIXED\"]  # This is the sum of the fixed model components\n",
    "draco = inputDict[\"DRACO\"]\n",
    "galdif = inputDict[\"GLL_IEM_V06\"]\n",
    "\n",
    "print \"Observed\",n_obs.sum()\n",
    "\n",
    "print \"Fixed Model\",fixed.sum()\n",
    "print \"Gal.Diff Model\",galdif.sum()\n",
    "print \"Draco Model\",draco.sum()\n",
    "print \"Gal + Fixed\",galdif.sum()+fixed.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have 23616 counts in our ROI, and the best-fit sum of the Galactic diffuse model and all the fixed model components adds up to 23705 counts.\n",
    "\n",
    "The counts for the Draco model are for a powerlaw spectrum with and index of -2 and an prefactor of $10^{-12} cm^{-2}s^{-1}MeV{-1}$ at 1000 GeV. \n",
    "\n",
    "We are going to be fitting for the scale factors of the Draco and Galactic diffuse components with respect to those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we make the list of the free models\n",
    "# the fixed component is handled seperately\n",
    "modelList = [ draco, galdif ] \n",
    "par_index = 0 # This is the index of the source we care about (i.e., Draco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are going to construct a function that will return the negative log-likelihood for a particular set of normailzation of the draco and Galactic Diffuse flux.\n",
    "\n",
    "There is utility function to do this in the [LikeFitUtils.py file](./LikeFitUtils.py) in this directory called NLL_func.   In terms of the objects we are using the formula for the negative log-likelihood is:\n",
    "\n",
    "$-\\log\\mathcal{L} = \\sum_{i} n_{i} \\log ( m_{i,\\rm{fixed}} + \\sum_{j} p_{j} m_{ij} ) - m_{i,\\rm{fixed}} - \\sum_{j} p_{j} m_{ij}$\n",
    "\n",
    "Where the index $i$ runs over all the energy bins and pixels, and $j$ run over the model compoents \n",
    "* $n_{i}$ are the observed counts: (n_obs)\n",
    "* $m_{i,\\rm{fixed}}$ are the preditcted counts of the fixed model component: (fixed)\n",
    "* $m_{ij}$ are the predicted counts of the free model components: [draco, galdif]\n",
    "* $p_{j}$ are the fit parameters: the will be the input to function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lfu.NLL_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftomin = lfu.NLL_func(n_obs,fixed,modelList)\n",
    "init_pars = np.ones((2))\n",
    "\n",
    "print \"NLL = %.1f\"%ftomin(init_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are going to want to minize the function.   I added a function to LikeFitUtils.py to do this using scipy.optimize.fmin, the function is called Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lfu.Minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, let's fit the function and parse out the results\n",
    "result = lfu.Minimize(ftomin,init_pars)\n",
    "mle_pars = result[0]\n",
    "mle = mle_pars[par_index]\n",
    "nll_min = result[1]\n",
    "nll_null = ftomin([0.,mle_pars[1]])\n",
    "TS = 2*(nll_null-nll_min)\n",
    "pvalue = 0.5 # fixme\n",
    "print \"Maximum likelihood estimate = %.1e cm-2 s-1 MeV-1\"%(mle*1e-12)\n",
    "print \"Minimum function value = %.2f\"%nll_min\n",
    "print \"Test Statistic = %.2f\"%TS\n",
    "print \"p-value = %.2f\"%pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so here we have seen that the best-fit value is $1.0^{-13}$cm$^{-2}$s$^{-1}$MeV$^{-1}$, but that the test statistic is only 0.47.  This tells us that we have not significantly detected emission from Draco with a powerlaw index of -2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Why does $TS = 0.47$ imply a non-detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of a detection, we are now going to derive *upper limits* on the powerlaw prefactor.   To do this are going to have to calculate the likelihood as a function of the Draco flux normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a likelihood scan\n",
    "par_bounds = (1e-2,2.0)\n",
    "nsteps = 25\n",
    "\n",
    "# Make a set of input parameter vectors that will serve to do the scan\n",
    "par_sets = lfu.MakeParSets_1DScan(mle_pars,par_index,par_bounds[0],par_bounds[1],nsteps,log=True)\n",
    "print par_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to scan likelihood two different ways.  \n",
    "1. Keeping the normalization of the other parameters fixed\n",
    "1. Re-optimizing the normalization of the other paramters are each scan step, this is called the profile likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells us to fix the Draco normalization during the Profile likelihood scan\n",
    "fix_par_mask = np.zeros((2),'?')\n",
    "fix_par_mask[par_index] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf1 = lfu.ParameterScan(ftomin,par_sets)\n",
    "print pf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's plot the likelihood, actually what we are going to plot is the \n",
    "# delta log-likelihood (w.r.t. the maximum)\n",
    "fig1,ax1 = lfu.PlotNLLScan(par_sets[0:,par_index],nll_min-pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case it doesn't really matter because the fit is simple, but it can be useful to interpolate the likelihood between the scan points rather that recompute it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp1 = lfu.BuildInterpolator(par_sets,par_index,pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to solve for the 95% confidence level upper limits.  In the likelihood ratio test where the test hypothesis has one extra degree of freedom, and we are at the boundary of that additional degree of freedom this occurs at the point that the log-likelihood is 1.35 less than maximum. \n",
    "\n",
    "We can derive that factor of 1.35 as follows.\n",
    "\n",
    "1. Chernoff's theorem states that if the null-hypothesis is correct then 50% of the trials will have TS=0 and the other 50% will be $\\chi^2$-distributed with one degree of freedom.\n",
    "1. *We assign the maximum likelihood estimate as the null-hypothesis.*\n",
    "1. For 1 degree of freedom the cumulative distribution function of the $\\chi^2$-distribution is simply $erf(\\sqrt{TS/2})$, and the p-value is complement of that, or $erfc(\\sqrt{TS/2})$.\n",
    "1. Therefore, in our case the p-value as a function x is $p = 0.5 erfc(\\sqrt{TS/2})$.\n",
    "1. Recall that TS is twice the delta log-likelihood, so the TS/2 is simply $\\Delta\\mathcal{L}$, and $p=0.5 erfc(\\sqrt{\\Delta\\mathcal{L}})$.\n",
    "1. We can solve numerically for $p=0.05$, or any other confidence level for that matter.\n",
    "1. For $p=0.05$ we get $\\Delta\\mathcal{L} = 1.3528$.\n",
    "\n",
    "Ok, let's solve for the upper limits now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lfu.SolveForErrorLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lim1 = lfu.SolveForErrorLevel(interp1,nll_min,1.35,mle,par_bounds)\n",
    "print \"Simple upper limit %.2e cm-2 s-1 MeV-1\"%(lim1*1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's make a plot of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig2,ax2 = lfu.PlotNLLScan(par_sets[0:,par_index],nll_min-pf1)\n",
    "\n",
    "ax2.hlines(-1.35,0,2.0,linestyles=u'dotted')\n",
    "ax2.vlines(lim1,-10,1,linestyles=u'dashed')\n",
    "    \n",
    "leg = ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed vertical line indicates the upper limit we have computed: 4.68e-13 cm-2 s-1 MeV-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile likelihood\n",
    "\n",
    "The potential flaw in what we have done is that we didn't allow any of the other paramters to vary as we scanned the log-likelihood as a function of the target flux.  To do things correctly we should at least allow the normalization of the diffuse background to vary to compenstate for the variation in the target flux.\n",
    "\n",
    "We will do this by re-optimizing the normalization of the other paramters are each scan step, this is called the profile likelihood.\n",
    "\n",
    "##### Question, how do we expect this to change our results?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lfu.ProfileScan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf2 = lfu.ProfileScan(n_obs, par_sets, fix_par_mask, fixed, modelList)\n",
    "interp2 = lfu.BuildInterpolator(par_sets,par_index,pf2)\n",
    "lim2 = lfu.SolveForErrorLevel(interp2,nll_min,1.35,mle,par_bounds)\n",
    "print \"Delta Log-Likelihood at 1.0e-12 cm-2 s-1 MeV-1 is %.1f\"%(interp2(1.0)-nll_min)\n",
    "print \"Profile upper limit %.2e cm-2 s-1 MeV-1\"%(lim2*1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, let's plot both sets of limits\n",
    "fig3,ax3 = lfu.PlotNLLScan(par_sets[0:,par_index],nll_min-pf1)\n",
    "ax3.plot(par_sets[0:,par_index],nll_min-pf2,'b-',label=\"Profile\")\n",
    "\n",
    "ax3.hlines(-1.35,0,2.0,linestyles=u'dotted')\n",
    "ax3.vlines(lim1,-10,1,linestyles=u'dashed')\n",
    "    \n",
    "leg = ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we can see several things:\n",
    "\n",
    "1. The simple scan and the profile scan are virtually indistinguishable.  This isn't too surprising, the Galactic diffuse background template didn't really look anything like the template for Draco.\n",
    "1. Both curves have a maximum at something like $1.0^{-13}$ cm$^{-2}$ s$^{-1}$ MeV$^{-1}$, since we are ploting the $\\Delta\\mathcal{L}$ the maximum value is 0.\n",
    "1. Both curves have a value of around 0.23 for zero Flux, taking twice that value gives us a TS = 0.47.\n",
    "\n",
    "#####  Question: Is there some way we could have saved ourselves the trouble of doing the profile scan?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "At this point we have performed a fit using all of energy bins from 500MeV to 500GeV, and we have tested DM source with a powerlaw spectrum with index -2 against the data.\n",
    "\n",
    "In a real dark matter search we have to consider many different spectra.  It would not be appropriate to use the fit results for this spectrum to constrain dark matter models with very different spectra.  \n",
    "\n",
    "We could imagine redoing the fitting with each different spectra we want to consider, however that is rather tedious.  Instead we would like to make a summary data product showing the flux in each of the energy bins and fit our dark matter spectra to that.\n",
    "\n",
    "The rest of this class will be about how we do that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** *[Back to the outline](FermiOverview.ipynb#Lecture-outline)* **\n",
    "\n",
    "** *[Forward to the SED fitting and limit setting example](SEDFittingExample.ipynb)* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

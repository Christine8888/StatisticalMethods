{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in Astronomy\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Work through example regression and classification analyses using the `scikit-learn` package and the SDSS catalog dataset.\n",
    "\n",
    "> Credit: much of the material in this chunk is based on Josh Bloom's SDSS example notebook, from the 2014 edition of \"Astro Hack Week\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometric Redshift Estimation and Quasar Classification\n",
    "\n",
    "Modern wide field surveys are generating very large databases of automatically measured objects, whose error properties may not be well understood. \n",
    "\n",
    "Fast machine learning algorithms are proving to be very useful in such a regime.\n",
    "\n",
    "Let's investigate the SDSS photometric object catalog, and look for machine learning solutions to the following two problems:\n",
    "\n",
    "1. Estimating the redshifts of quasars from their photometry (regression)\n",
    "\n",
    "2. Selecting quasars from a background of stars and galaxies (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\">\n",
    " <pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE \n",
    " (class = 'QSO')\n",
    " ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "We've got 1000 stars and 1000 galaxies as well, and saved them for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For pretty plotting\n",
    "# !pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometric Redshift Estimation\n",
    "\n",
    "* This is a regression problem, to be able to predict the redshift response variable given a number of photometric measurement \"features\"\n",
    "\n",
    "\n",
    "* The SDSS spectroscopic quasar sample provides a _training set_ for the prediction of photometric redshifts\n",
    "\n",
    "\n",
    "* Let's read in our SDSS quasars, and munge them into machine learning inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qsos = pd.read_csv(\"../examples/SDSSCatalog/data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visual Exploration\n",
    "\n",
    "* Over what redshift range do we have quasar spectra?\n",
    "\n",
    "* What structure is there in the _photometric feature space_ that might help us predict redshift?\n",
    "\n",
    "\n",
    "Let's plot all the features, colored by the target redshift, to look for structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bins =  hist(qso_redshifts.values,bins=100) ; xlabel(\"redshift\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(qso_features[0:2000], alpha=0.2, figsize=[15,15], c=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Data and Target Values\n",
    "\n",
    "Now we have our machine learning inputs (photometric features: colors and magnitudes) and outputs (target redshift values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = qso_features.values  # Data: 9-d feature space\n",
    "y = qso_redshifts.values # Target: redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's follow the same procedure as in the [`SciKit-Learn` Linear Regression tutorial](../../scikit-learn/Linear_Regression.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set:\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Instantiate a \"linear model\"\n",
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do? Compute MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_linear = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "r2_linear = linear.score(X_test, y_test)\n",
    "print(\"Linear regression: MSE = \",mse_linear)\n",
    "print(\"R2 score =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test,y_lr_pred - y_test,'o',alpha=0.2)\n",
    "title(\"Linear Regression Residuals - MSE = %.2f\" % mse_linear)\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Residual\")\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())\n",
    "print(\"Linear regression: MSE = \",mse_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-Nearest Neighbor (KNN) Regression\n",
    "\n",
    "Now let's try a different kind of model: a *non-parametric* one.\n",
    "\n",
    "[\"Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "\n",
    "#### Question:\n",
    "\n",
    "What underlying model is implied by the KNN algorithm? How many hidden parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X) # Many methods work better on scaled X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "KNN.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test)\n",
    "mse_knn = mean_squared_error(y_test,y_knn_pred)\n",
    "r2_knn = KNN.score(X_test, y_test)\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_knn_pred - y_test,'o',alpha=0.2)\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse_knn)\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Residual\")\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's vary the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "> KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
    "\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# And we'll need a cv iterator:\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(len(X), 10, test_size=0.4)\n",
    "\n",
    "# Compute our cv scores for a range of the no. of neighbors:\n",
    "from sklearn.learning_curve import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "Can you explain the shapes of these two curves? Talk to your neighbor for a few minutes, and be prepared to suggest reasons for a) the rise and fall of the cross validation score and b) the monotonic decrease in training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model tuning with `GridSearchCV`\n",
    "\n",
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "KNN_tuned = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `GridSearchCV` object behaves just like a model, except it carries out a cross-validation while fitting:\n",
    "\n",
    "<img src=\"../../scikit-learn/figures/grid_search_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_tuned_pred = KNN_tuned.predict(X_test)\n",
    "\n",
    "mse_knn_tuned = mean_squared_error(y_test,y_knn_tuned_pred)\n",
    "r2_knn_tuned = KNN_tuned.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\",r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) = \",mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the best KNN control parameters we found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value of `n_neighbors` is consistent with the peak in cross-validation score in the validation curve plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "\n",
    "Notice that all the above tuning happened while training on a single split (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "It's possible that that particular fold prefers a slightly different set of parameters than a different one - so to assess our generalization error, we need a further level of cross-validation.\n",
    "\n",
    "\n",
    "We can do this by passing a `GridSearchCV` model to the cross validation score calculator. This will take a few moments, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(KNN_tuned, X_scaled, y, cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanR2,errR2 = np.mean(R2),np.std(R2)\n",
    "print(\"Mean score:\",meanR2,\"+/-\",errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Optimizing over control parameters (or hyper parameters) with grid search cross validation is a form of model selection.\n",
    "\n",
    "\n",
    "* When presented with new data samples (photometry), and asked to predict the target response variables (photometric redshifts), we'll need a trained machine that has not been *over-fitted* to the training data.\n",
    "\n",
    "\n",
    "* Minimizing and estimating the generalization error is a way to reduce the risk of getting this prediction wrong. \n",
    "\n",
    "\n",
    "* Let's finish off our photo-z machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNNz = KNN_tuned.best_estimator_\n",
    "KNNz.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 571\n",
    "one_pretend_quasar = X_test[j,:]\n",
    "zphoto = KNNz.predict(one_pretend_quasar)\n",
    "zspec = y_test[j]\n",
    "print(\"True redshift cf. KNN photo-z:\",zspec,zphoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zspec = y_test\n",
    "zphoto = KNNz.predict(X_test)\n",
    "\n",
    "plot(zspec, zphoto,'o',alpha=0.1)\n",
    "title(\"KNNz performance\")\n",
    "xlabel(\"Spectroscopic Redshift\")\n",
    "ylabel(\"Photometric redshift\")\n",
    "lims = [0.0,4.0]\n",
    "xlim(lims)\n",
    "ylim(lims)\n",
    "plot(lims, lims, ':k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasar Classification with Random Forests\n",
    "\n",
    "\n",
    "* Let's switch gears and do a 3-class classification problem: star, galaxy, or QSO.\n",
    "\n",
    "\n",
    "* A very good general-purpose classification (and regression!) algorithm is Random Forest. See [this blog post](http://blog.yhathq.com/posts/random-forests-in-python.html) for a nice high level introduction.\n",
    "\n",
    "\n",
    "* [\"A random forest is a meta estimator that fits a number of *decision tree classifiers* on various sub-samples of the dataset, and uses averaging to improve the predictive accuracy and control over-fitting.](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "* Let's read in equal numbers of all three types of data, clean them up, and set $y$ equal to the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sources = pd.read_csv(\"data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"])[:1000]\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/star1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/galaxy1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources[(all_sources[\"dered_r\"] > -9999) & (all_sources[\"g_r_color\"] > -10) & (all_sources[\"g_r_color\"] < 10)]\n",
    "\n",
    "all_labels = all_sources[\"class\"]\n",
    "\n",
    "all_features = copy.copy(all_sources)\n",
    "del all_features[\"class\"]\n",
    "\n",
    "X = copy.copy(all_features.values)\n",
    "y = copy.copy(all_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape =\", X.shape)\n",
    "print(\"Class label vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What structure can we see in the data? Let's plot all the features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = all_labels.values.copy()\n",
    "yy[yy==\"QSO\"] = 0.0    # Red\n",
    "yy[yy==\"STAR\"] = 0.5   # Green\n",
    "yy[yy==\"GALAXY\"] = 1.0 # Blue\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(yy), vmax=max(yy))\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "rez = pd.scatter_matrix(all_features,alpha=0.2,figsize=[15,15],color=m.to_rgba(yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - looks like there is information there to be used! \n",
    "Let's turn on the machine learning.\n",
    "\n",
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,oob_score=True)\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the important features in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(all_sources.columns.values,rf.feature_importances_),key=lambda q: q[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"Out of Bag\" accuracy (of predicted y compared to truth), made available by ensemble classifiers. (Each decision tree in the ensemble is only working on a subset of the data, so it can track its accuracy with the data not in its own bag.) \n",
    "\n",
    "The accuracy of a classifier is the fraction of predictions made that are correct. This one looks like its doing well - but this is the accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier improvement with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameter values to try:\n",
    "parameters = {'n_estimators':(50,100,200),\"max_features\": [\"auto\",3],\n",
    "              'criterion':[\"gini\",\"entropy\"],\"min_samples_leaf\": [1,2]}\n",
    "\n",
    "# Initial training/test split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a grid search to find the highest 3-fold CV score:\n",
    "rf_tuned = GridSearchCV(rf, parameters, cv=3, verbose=1)\n",
    "RFselector = rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and estimator:\n",
    "print(RFselector.best_score_)\n",
    "print(RFselector.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "Would you be satisfied with a 95% successful classification fraction? Read the Random Forest `SciKit-Learn` docs to find some alternative scores, and think about when you might want to choose one of these instead. (Hint: imagine using a classifier to select a sample of *targets*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of visualizing classification accuracy is via a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = RFselector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output label comes with a *classification probability*, computed from the results of the whole forest. To select a sample of classified objects, one can choose a selection threshold in this class probability, and only keep objects with higher probability than this threshold.\n",
    "\n",
    "\n",
    "The availability of a class probability leads to an important diagnostic: the \"Receiver Operating Characteristic\" or [\"ROC\" curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). This shows the *true positive rate* (TPR) plotted against the *false positive rate* (FPR) of a classifier, as the selection threshold is varied.\n",
    "\n",
    "\n",
    "Typically, classifiers have control parameters that affect both the TPR and FPR (often improving one at the expense of the other), so the ROC curve is a good tool for investigating these parameters. \n",
    "\n",
    "\n",
    "Likewise, ROC curves provide a very good way to compare different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Use `SciKit-Learn` utilities to plot an ROC curve for the RFselector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Back to the lesson plan](../../lessons/9.MachineLearning.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

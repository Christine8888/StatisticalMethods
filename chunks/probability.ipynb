{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of Probability\n",
    "\n",
    "Goals:\n",
    "* Review the bits of mathematical probability that will be most important for us later.\n",
    "* Prime our brains for probabilistic reasoning.\n",
    "\n",
    "This will be quick -- the key concepts will already be familiar if you've studied statistical and/or quantum mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some terminology\n",
    "* Sample space ($\\Omega$): the set of all possible answers/outcomes for a given question/experiment.\n",
    "* Event ($E$): any subset of $\\Omega$.\n",
    "\n",
    "The probability of an event will be a real function satisfying certain requirements..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Axioms of probability:\n",
    "* $\\forall E: 0 \\leq P(E) \\leq 1$\n",
    "* $P(\\Omega) = 1$\n",
    "* If $E_i$ are mutually exclusive, $P\\left(\\bigcup E_i\\right) = \\sum_i P(E_i)$\n",
    "\n",
    "This dry definition provides a function with the right properties to describe our intuitive understanding of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A familiar example\n",
    "\n",
    "Let $\\Omega$ be the set of states available to a system of fixed energy, e.g. a box full of gas particles.\n",
    "\n",
    "With one additional (physics) assumption, that it's equally probable for the system to occupy any state in $\\Omega$, this is the microcanonical ensemble in statistical mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete vs. continuous sample spaces\n",
    "Very often the type of event we're interested in lives in a continuous sample space, e.g. the event $h=0.7$.\n",
    "\n",
    "Our axioms mostly translate straightforwardly; in this example $P(\\Omega)=1$ becomes the normalization condition\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} p(h=x)dx$ = 1\n",
    "\n",
    "We can always describe the discrete case as a continuous one where $p$ is a sum of Dirac delta functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More definitions\n",
    "If $X$ takes real values, then $p(X=x)$ is a **probability density** function, or PDF.\n",
    "* $p(X=x)$ is *not* a probability! But integrals like $p(X=x)dx$ and $P(x_0 < X < x_1)$ are.\n",
    "* We will rapidly become lazy and denote $p(X=x)$ incorrectly as $P(X)$. You have been warned.\n",
    "\n",
    "The first bullet is highly relevant if we ever want to change variables, e.g. $x\\rightarrow y(x)$\n",
    "* $P(y) \\neq P[x(y)]$; rather $P(y) = P(x)~\\left|dx/dy\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Even more definitions\n",
    "The **cumulative distribution function** (CDF) is the probability that $X\\leq x$.\n",
    "* Usually written: $F(x) = P(X \\leq x) = \\int_{-\\infty}^x p(X=x')dx'$.\n",
    "* Conversely, the PDF is the derivative of the CDF.\n",
    "* (The CDF is sometimes referred to just as the distribution function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Yet more definitions\n",
    "The **quantile function** is the inverse of the CDF, $F^{-1}(P)$.\n",
    "* Multiply a quantile by 100 and it's a percentile.\n",
    "* The median of a distribution $F$ is $F^{-1}(0.5)$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridiculous example: an unfair coin toss\n",
    "We flip a coin which is weighted to land on heads a fraction $q$ of the time. To make things numeric, let $X=0$ for stand for an outcome of tails and $X=1$ for heads.\n",
    "\n",
    "X | $~~~$PDF$~~~$     | $~~~$CDF$~~~$\n",
    "--- | :-------: | :---:\n",
    "0 |  $1-q$    |  $1-q$\n",
    "1 | $q$ |  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Joint probability distributions\n",
    "Things get more interesting when we deal with joint distributions of multiple events, $P(X=x$ and $Y=y)$, or just $P(X,Y)$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/prob_joint_correlated.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **marginal probability** of $Y$, $P(Y)$, means the probability of $Y$ *irrespective* of what $X$ is.\n",
    "* $P(Y) = \\int dX ~ P(X,Y)$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/prob_joint_correlated.png\" width=400></td>\n",
    "        <td><img src=\"../graphics/prob_joint_marginal.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **conditional probability** of $Y$ *given* a value of $X$, $P(Y|X)$, is most easily understood this way\n",
    "* $P(X,Y) = P(Y|X)~P(X)$\n",
    "\n",
    "i.e., $P$ of getting $X$ AND $Y$ can be decomposed into\n",
    "* $P$ of getting $X$ regardless of $Y$, and\n",
    "* $P$ of getting $Y$ given $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$P(Y|X)$ is a (normalized) slice through $P(X,Y)$ rather than an integral.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/prob_joint_correlated.png\" width=400></td>\n",
    "        <td><img src=\"../graphics/prob_joint_conditional.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$X$ and $Y$ are **independent** if $P(Y|X) = P(Y)$.\n",
    "\n",
    "Equivalently, if $P(X,Y) = P(X)~P(Y)$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../graphics/prob_joint_independent.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "Take the coin tossing example from earlier, where $P(\\mathrm{heads})=q$ and $P(\\mathrm{tails})=1-q$ for a given toss. Assume that this holds independently for each toss.\n",
    "\n",
    "Find\n",
    "\n",
    "1. The conditional probability that both tosses are heads given that the first toss is heads\n",
    "2. The conditional probability that both tosses are heads given that at least one of the tosses is heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "Say we keep on tossing this coin, still assuming independence, a total of $N$ times. Work out the probability that exactly $n$ of these turn out to be heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to count things\n",
    "The answer to the previous exercise is the PDF of the binomial distribution\n",
    "\n",
    "$P(n~|~q,N) = {N \\choose n} q^n (1-q)^{N-n}$\n",
    "\n",
    "To introduce some notation, we might write this as\n",
    "\n",
    "$n \\sim \\mathrm{Binom}(q,N)$\n",
    "\n",
    "Here the squiggle means \"$n$ is a random variable that follows this distribution\", rather than \"this is a hand-wavy statement\" (the common usage in physics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to count things\n",
    "Recall that a key assumption was that each toss (trial) was independent. If let $\\mu=qN$ and also assume that $q$ is small while $N$ is large, then a series of irritating limits and substitutions yields the Poisson distribution\n",
    "\n",
    "### $P(n|\\mu) = \\frac{\\mu^n e^{-\\mu}}{n!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to count things\n",
    "This is a ridiculously important result, given that most astronomy and physics experiments boil down to counting events that are rare compared with the number of time intervals in which they might happen (and be recorded).\n",
    "* E.g., most obviously, the number of photons from some source hitting a particular CCD pixel during an observation.\n",
    "\n",
    "It has the following (probably familiar) properties:\n",
    "* Expectation value (mean) $\\langle n\\rangle = \\mu$\n",
    "* Variance $\\left\\langle \\left(n-\\langle n \\rangle\\right)^2 \\right\\rangle = \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus exercise\n",
    "Test your understanding of the basic mathematical tools of probability by proving the formula for transforming random variables\n",
    "\n",
    "$P(y) = P(x)~\\left|\\frac{dx}{dy}\\right|$\n",
    "\n",
    "for the case where $y(x)$ is monotonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus numerical exercise\n",
    "Go farther with the previous exercise! Consider the function $b=tan(\\theta)$, which is sometimes used to reparametrize the slope of a line ($b$) with the angle the line makes in a plot ($\\theta$).\n",
    "1. If $P(\\theta)$ is uniform (proportional to a constant) for $-\\pi\\theta<\\pi$, work out $P(b)$.\n",
    "2. Demonstrate that you're right by generating a bunch of uniform random $\\theta$'s, transforming them to $b$'s, and comparing a histogram of them with your answer to (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference Example: Fitting a Straight Line to the Cepheid Data\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Apply some of the principles discussed in the [\"Bayes Theorem\"](bayes_theorem) and [\"Generative Models\"](generative_models.ipynb) notebooks to a simple example problem.\n",
    "\n",
    "* Carry out a Bayesian inference, deriving, coding up, evaluating, visualizing and summarizing a posterior PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* [Hogg, Bovy & Lang (2010), \"Data analysis recipes: Fitting a model to data\"](https://arxiv.org/abs/1008.4686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A period-magnitude relation in Cepheid stars\n",
    "\n",
    "* Cepheids star brightness oscillates with a stable period that appears to be strongly correlated with their mean luminosity (or absolute magnitude).\n",
    "\n",
    "* In the [\"cepheids\"](cepheids.ipynb) notebook we looked at some Cepheid measurements reported by [Riess et al (2011)](https://arxiv.org/abs/1103.2976).\n",
    "\n",
    "* Let's infer the parameters of a simple relationship between Cepheid period and, in the first instance, apparent magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/cepheid_data.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "* Let's assume that Cepheid stars' luminosities are related to their oscillation periods by a power law, such that their apparent magnitude and log period follow the straight line relation\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m = a\\;\\log_{10} P + b$\n",
    "\n",
    "* Our task is to infer the parameters $a$ and $b$ given the data, which consist of *observed magnitudes with quoted uncertainties*, such as: \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m^{\\rm obs} = 24.51 \\pm 0.31$ at $\\log_{10} P = \\log_{10} (13.0/{\\rm days})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "* \"Inferring the parameters $a$ and $b$\" means characterizing the posterior PDF for these parameters, given the data we have $(\\boldsymbol{m}^{\\rm obs})$ and the assumptions we make $(H)$.\n",
    "\n",
    "* In mathematics: we seek the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "* From Bayes Theorem, or equivalently, the alternative factorization of the joint PDF for all variables, we have that \n",
    "\n",
    "\n",
    "### $\\;\\;\\;\\;\\;{\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H) = \\frac{{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a,b|H)}{{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|H)}$\n",
    "\n",
    "* ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|H)$ is, in this context, a normalization constant\n",
    "\n",
    "* If we can evaluate the numerator of the righthand side of the above expression, we can compute the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$ for any choice of parameters $(a,b)$, up to this constant.\n",
    "\n",
    "* This numerator is just the factorization of the joint PDF for all variables that the PGM illustrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic Graphical Model\n",
    "\n",
    "* Let's draw a PGM for this inverse problem, imagining our way through what we would do to generate a mock dataset like the one we have for each of the 9 NGC galaxy hosts in R11.\n",
    "\n",
    "* If we were generating mock data, then for any plausible choice of parameters $a$ and $b$ we can predict the true magnitude $m_k$ of each star given its period $P_k$, and then add noise to simulate each observed magnitude $m^{\\rm obs}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PGM\n",
    "\n",
    "<img src=\"../graphics/pgms_cepheids.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaching inference\n",
    "\n",
    "* Recall: the PGM illustrates a particular factorization of the joint PDF for all variables in the problem - the one dictated by our model assumptions\n",
    "\n",
    "* In an inverse problem, the data $m^{\\rm obs}$ are constants, fixed by observation\n",
    "\n",
    "* Other parameters of the model are fixed by assumption\n",
    "\n",
    "> The magnitude uncertainties $\\sigma^{\\rm obs}$ are given to us in the data file; we can use them as-is if we believe them. The \"true\" magnitudes $m$ are _determined_ by our power law model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaching inference\n",
    "\n",
    "* Let's write down the joint PDF corresponding to the above PGM, paying attention to the assumptions involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The sampling distribution ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},H)$\n",
    "\n",
    "* We were given points ($m^{\\rm obs}_k$) with error bars ($\\sigma_k$), which suggests a *Gaussian* sampling distribution for each one:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp{-\\frac{(m^{\\rm obs}_k - m_k)^2}{2\\sigma_k^2}}$\n",
    "\n",
    "\n",
    "* Then, we might suppose that the measurements of each Cepheid start are *independent* of each other, so that we can define *predicted and observed data vectors* $\\boldsymbol{m}$ and $\\boldsymbol{m}^{\\rm obs}$ (plus a corresponding observational uncertainty vector $\\boldsymbol{\\sigma}$) via:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\boldsymbol{\\sigma},H) = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional PDF ${\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H)$\n",
    "\n",
    "<img src=\"../graphics/pgms_cepheids.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional PDF ${\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H)$\n",
    "\n",
    "Our relationship between the intrinsic magnitude and the log period is linear and deterministic, indicating the following *delta-function* PDF:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H) = \\delta(m_k - a\\log_{10}{P_k} - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The resulting joint likelihood, ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "* The PDF for everything inside the plate is the product:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;{\\rm Pr}(\\boldsymbol{m}|a,b,H)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;= \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log_{10}{P_k} - b)$\n",
    "\n",
    "* The intrinsic magnitudes of each Cepheid $m_k$ are not interesting, and so we _marginalize them out_:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\int {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;{\\rm Pr}(\\boldsymbol{m}|a,b,H)\\; d\\boldsymbol{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The resulting joint likelihood, ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "* We are left with ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\prod_k {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The log likelihood\n",
    "\n",
    "Taking logs, the product in the joint likelihood becomes the following sum:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\sum_k \\log {\\rm Pr}(m^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma,H)$\n",
    "\n",
    "which, substituting in our Gaussian form, gives us: \n",
    "\n",
    "$\\;\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = -\\frac{1}{2}\\sum_k \\log{2\\pi\\sigma_k^2} - \\frac{1}{2} \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "> Note that $\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$ can be evaluated, as a function of $a$ and $b$, at constant $\\boldsymbol{m}^{\\rm obs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chi-squared\n",
    "\n",
    "* Astronomers often call the term in the log likelihood that depends on the parameters $\\chi^2$ (\"chi-squared\"):\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\chi^2 = \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "* $\\chi^2$ is a \"misfit\" statistic, that quantifies the difference between \"observed and predicted data.\" Under our assumptions, it's equal to -2 times the log likelihood (up to a constant). The \"predicted data\" are $m_k = a\\log{P_k} - b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Including the prior ${\\rm Pr}(a,b|H)$\n",
    "\n",
    "* The final pieces of the joint PDF illustrated by the PGM are the PDFs for $a$ and $b$\n",
    "<img src=\"../graphics/pgms_cepheids.png\" width=60% align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Including the prior ${\\rm Pr}(a,b|H)$\n",
    "\n",
    "* The final pieces of the joint PDF illustrated by the PGM are the PDFs for $a$ and $b$, which we can assume to be independent\n",
    "\n",
    "* The joint PDF is:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs},a,b|H) = {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a|H) {\\rm Pr}(b|H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Since we marginalized out the $m$, analytically, we _could_ have drawn the PGM more simply, jumping directly to ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$. However, it's often helpful to _explicitly_ distinguish between \"true\" parameters and observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigning the prior\n",
    "\n",
    "* For now, let's assume a uniform prior PDF for $b$, supposing that we know roughly what size $b$ is (about 20).\n",
    "\n",
    "* Since $a$ is the gradient of a line, let's assume a uniform prior in the angle of inclination $\\theta$ of the line. With $a = \\tan{\\theta}$, this choice corresponds to a Cauchy distribution for $a$.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(a|H) = \\frac{1}{1+a^2}\\;\\;{\\rm for}\\;\\; -\\infty < a < +\\infty$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(b|H) = \\frac{1}{b_{\\rm max} - b_{\\rm min}}$ with $(b_{\\rm min}, b_{\\rm max}) = (10, 30)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "* With this completed factorization of the joint PDF for all variables, we have that \n",
    "\n",
    "$\\;\\;{\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H) \\propto {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a|H) {\\rm Pr}(b|H)$\n",
    "\n",
    "* This means that we can evaluate the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$ for any choice of parameters $(a,b)$, up to a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the posterior PDF\n",
    "\n",
    "We can now code up functions for the log likelihood, the log prior, and the unnormalized log posterior, such that we can evaluate them on a 2D $(a,b)$ parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('../code/cepheids.py').read())\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "data = Cepheids('../examples/Cepheids/R11ceph.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(logP, mobs, sigma, a, b):\n",
    "    return -0.5*np.sum(2*np.pi*sigma**2) - \\\n",
    "            0.5*np.sum((mobs - a*logP -b)**2/(sigma**2))\n",
    "\n",
    "def log_prior(a, b):\n",
    "    amin,amax = -10.0,10.0\n",
    "    bmin,bmax = 10.0,30.0\n",
    "    if (b > bmin)*(b < bmax):\n",
    "        logp = np.log(1.0/(amax-amin)) + \\\n",
    "               (1/np.pi)*np.log(1.0/(1 + b**2))\n",
    "    else:\n",
    "        logp = -np.inf\n",
    "    return logp\n",
    "\n",
    "def log_posterior(logP, mobs, sigma, a, b):\n",
    "    return log_likelihood(logP,mobs,sigma,a,b) + log_prior(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the posterior PDF\n",
    "\n",
    "Now, let's set up a suitable parameter grid, evaluate the unnormalized log posterior on it, and then renormalize it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select a Cepheid dataset:\n",
    "data.select(4258)\n",
    "\n",
    "# Set up parameter grids, focused on the high likelihood region:\n",
    "npix = 100\n",
    "amin,amax = -3.4,-2.4\n",
    "bmin,bmax = 25.7,26.8\n",
    "agrid, bgrid, logprob = np.linspace(amin,amax,npix), np.linspace(bmin,bmax,npix), np.zeros([npix,npix])\n",
    "\n",
    "# Loop over parameters, computing unnormlized log posterior PDF:\n",
    "for i,a in enumerate(agrid):\n",
    "    for j,b in enumerate(bgrid):\n",
    "        logprob[j,i] = log_posterior(data.logP,data.mobs,data.sigma,a,b)\n",
    "\n",
    "# Exponentiate and normalize to get posterior density:\n",
    "prob = np.exp(logprob - np.max(logprob))\n",
    "prob /= np.sum(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the 2D PDF\n",
    "\n",
    "* Typically we want to be able to see the centroid, size and shape of the posterior PDF\n",
    "\n",
    "* In particular we want to see the _credible regions_ that enclose 68% and 95% of the posterior probability. These are best plotted as contours\n",
    "\n",
    "* Given our assumption that the model is true, the probability that the true values of the model parameters lie within the 95% credible region given the data is 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the 2D PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sorted = np.sort(prob.flatten())\n",
    "C = sorted.cumsum()\n",
    "\n",
    "# Find the pixel values that lie at the levels that contain 68% and 95% of the probability:\n",
    "lvl68 = np.min(sorted[C > (1.0 - 0.68)])\n",
    "lvl95 = np.min(sorted[C > (1.0 - 0.95)])\n",
    "\n",
    "plt.imshow(prob, origin='lower', cmap='Blues', interpolation='none', extent=[amin,amax,bmin,bmax])\n",
    "plt.contour(prob,[lvl95,lvl68],colors='black',extent=[amin,amax,bmin,bmax])\n",
    "plt.grid()\n",
    "plt.xlabel('slope a', fontsize=20)\n",
    "plt.ylabel('intercept b / AB magnitudes', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-inference model checking\n",
    "\n",
    "\n",
    "* Are these inferred parameters sensible? \n",
    "\n",
    "* Let's read off a plausible (a,b) pair and overlay the model period-magnitude relation on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "data.overlay_straight_line_with(a=-3.0, b=26.35)\n",
    "\n",
    "data.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "* Typically, we will want to (or will be expected to) report \"answers\" for our model parameters\n",
    "\n",
    "* This can be difficult: our result _is_ the posterior PDF for the model parameters given the data!\n",
    "\n",
    "* A convenient, and in this case appropriate, choice is to report quantiles of the 1D marginalized PDFs\n",
    "\n",
    "> In general, the most important thing when summarizing inferences is to state clearly what you are doing, preferably with critical commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "Let's compute the 1D marginalized posterior PDFs for $a$ and for $b$, and report the median and \"68% credible interval\" (defined as the region of 1D parameter space enclosing 68% of the posterior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_a_given_data = np.sum(prob, axis=0) # Approximate the integral as a sum\n",
    "prob_b_given_data = np.sum(prob, axis=1) # Approximate the integral as a sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that we do have a 1D PDF:\n",
    "print(prob_a_given_data.shape, np.sum(prob_a_given_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D marginalized posterior PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_1d_marginalized_pdfs(agrid, bgrid, prob_a_given_data, prob_b_given_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D marginalized PDF summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"a = \",compress_1D_pdf(agrid, prob_a_given_data, ci=68, dp=2))\n",
    "\n",
    "print(\"b = \",compress_1D_pdf(bgrid, prob_b_given_data, ci=68, dp=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "* In this simple case, our report makes sense: the medians of both 1D marginalized PDFs lie within the region of high 2D posterior PDF. *This will not always be the case.*\n",
    "\n",
    "\n",
    "* The marginalized posterior for $x$ has a well-defined meaning, regardless of the higher dimensional structure of the joint posterior:  it is ${\\rm Pr}(x|d,H)$, the PDF for $x$ given the data and the model, and *accounting for the uncertainty in all other parameters*.\n",
    "\n",
    "\n",
    "* The high degree of symmetry in this problem is due to the posterior being a *bivariate Gaussian.* We could have derived the posterior PDF *analytically* - but in general this will not be possible."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}


## Evaluating the posterior PDF

Evaluating the log likelihood, and hence the log posterior, is linear in $N$, the number of datapoints.

The parameter grid is a $P$-dimensional array, $q$ pixels on a side for a total of $q^P$ cells.

The total number of flops, and the CPU time, will therefore scale as $t \sim N q^P$, very approximately. It scales _exponentially_ with the dimensionality of the parameter space.



## Marginalizing the posterior PDF

* What does it take to make just a single one-dimensional marginalized PDF?

* Starting from a $P$-dimensional hypercube, the first integral involves looping over $q^{P-1}$ hypercolumns, and doing a sum over $q$ elements in each one, for a subtotal of $\sim q^P$ operations, roughly. The second marginalization integral will involve $q^{P-2}$ hypercolumns for a subtotal of $\sim q^{P-1}$ operations, and so on.

* These subtotals then need to be summed to count the total number of operations. The integral over 1 column we don't have to do (because we want the 1D PDF, not the 0D summary), so we have $t \sim \sum_{i=2}^P q^i$. This sum will be dominated by the final term, corresponding to the highest dimensional integral: $t \sim q^P$

* To make all $P$ one-dimensional marginalized PDFs is just $P$ times this result: $t \sim P q^P$. More efficient algorithms than this exist, that re-use some of the integrations, but this basic scaling is probably not far off.

* Even though all we are doing is summing, not evaluating any expensive likelihood, the cost of marginalization can be significant: it too scales exponentially with the dimensionality of the parameter space.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This notebook is adapted from [a lesson from the 2017 KIPAC/StatisticalMethods course](https://github.com/KIPAC/StatisticalMethods/blob/2017winter/chunks/generative_models.ipynb), (c) 2017 Adam Mantz and Phil Marshall. The GPLv2 license applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models\n",
    "\n",
    "Goals:\n",
    "* Introduce generative models in the context of mocking data and inference\n",
    "* Introduce probabilistic graphical models as a tool for model visualization\n",
    "* Practice building some simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Ivezic et al, Sections 3.3 and 3.7\n",
    "* [Bishop, \"Pattern Recognition and Machine Learning,\"](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738) Sections 8.1 and 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **generative model** formalizes our understanding of how a data set comes to exist, including\n",
    "* physical processes happening out there in the Universe\n",
    "* instrumental effects and the measurement process\n",
    "* any computations done prior to calling the result a \"data set\"\n",
    "\n",
    "In other words, it's what we need in order to generate a mock data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To actually generate mock data, we need to specify the **sampling distribution**, $P(\\mathrm{data}|\\mathrm{model})$. This PDF is the mathemetical expression of our generative model.\n",
    "* It shows up directly in Bayes Theorem, and ideally (as a function of model parameters) as the likelihood function in maximum-likelihood fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are generative models useful for?\n",
    "\n",
    "* Performing inference: constructing the *sampling distribution* or *likelihood function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Testing inference: does our analysis, run on mock data, recover the input model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Checking inferences: do mock data generated from a fitted model resemble the real data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The Sampling Distribution\n",
    "\n",
    "* The noisy, mock data are drawn from a PDF known as the \"sampling distribution\"\n",
    "\n",
    "* In the case of an X-ray image pixel, containing mock counts $N_k$, this PDF is $P(N_k|\\mu_k,H)$\n",
    "\n",
    "* $\\mu_k$ is a parameter, the expected number of counts in the $k^{th}$ pixel\n",
    "\n",
    "* $H$ is the list of assumptions that defines our model\n",
    "\n",
    "> The sampling distribution is sometimes called the error distribution, because it captures the statistical properties of the \"random errors\" in the data. Drawing from the sampling distribution \"adds noise\" to the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Conditional probability\n",
    "\n",
    "* $P(N_k|\\mu_k,H)$ is pronounced \"the probabilty for $N_k$ given $\\mu_k$ and $H$\"\n",
    "\n",
    "* This means that if you know what the value of $\\mu_k$ is, you can _draw_ a sample value of $N_k$ from $P$ - since  $H$ tells you what the functional form of $P$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sampling in practice\n",
    "\n",
    "* In general, sampling from a PDF is difficult\n",
    "\n",
    "* For certain standard distributions, however, there are fast algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "P = scipy.stats.poisson(mu=3)\n",
    "P.rvs(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sampling in practice\n",
    "\n",
    "* [numpy.random]() and [scipy.stats]() are two useful libraries for drawing samples from PDFs\n",
    "\n",
    "* You may have encountered some of these routines as \"random number generators\"\n",
    "\n",
    "* Sampling from a PDF means generating random numbers from that distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Choosing the input model parameters\n",
    "\n",
    "* When testing, we often want to assert a set of input model parameters $\\theta$, and then see what they produce\n",
    "\n",
    "* Testing at large scale might involve generating many datasets, with different inputs. In this case we might want to sample from a *plausible distribution of input parameters*\n",
    "\n",
    "* In practice: choose a particular standard probability distribution for $\\theta$ and sample from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Deterministic relationships\n",
    "\n",
    "* Often, our model provides a deterministic relationship between parameters: if you know $\\theta$, then you know $\\mu$\n",
    "\n",
    "* In our X-ray image case, $\\theta$ could be the set of parameters that describe the gas temperature and density profiles of a spherically symmetric cluster of galaxies with known centroid position\n",
    "\n",
    "* $\\mu(\\theta)$ would then be some complicated function that took the cluster parameters $\\theta$ as input, and predicted the expectation value of the counts at any pixel position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **probabilistic graphical model** (PGM) is a very useful way of visualizing a generative model.\n",
    "* They sketch out the procedure for how one would generate mock data in practice.\n",
    "* They illustrate the interdependence of model parameters, and the dependence of data on parameters.\n",
    "* They also (therefore) represent a conditional factorization of the sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Many, many** mistakes can be avoided by sketching out a PGM at the outset of a statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Technically, a PGM is a type of *directed acyclic graph*, where **nodes** and **edges** represent parts of the model.\n",
    "\n",
    "Let's look at a very simple example..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table><tr width=90%>\n",
    "<td><img src=\"../graphics/tour_cluster_image.png\" height=300></td>\n",
    "<td><img src=\"../graphics/tour_cluster_image_zoom.png\" height=300></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our measurement is the number of counts in each pixel. Here is a generative model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There's an object emitting light, whose properties are parametrized by $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* From $\\theta$, we can determine the average flux falling on a given pixel $k$, $F_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given the exposure time of our observation, $T$, and some conversion factors, $F_k$ determines the average number of counts expected, $\\mu_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The number of counts measured, $N_k$, is a Poisson draw, given the average $\\mu_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the model was described in terms of conditional relationships.\n",
    "* $\\theta \\Rightarrow F_k$\n",
    "* $F_k,T \\Rightarrow \\mu_k$\n",
    "* $N_k \\sim \\mathrm{Poisson}(\\mu_k)$\n",
    "\n",
    "The PGM will do the same, visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/pgms_pixelcounts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ingredients of a PGM:\n",
    "* **Nodes** represent parameters\n",
    "* **Edges** represent conditional relationships\n",
    "* **Plates** represent repeated model components whose contents are conditionally independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Types of nodes:\n",
    "* **Circles** represent a PDF. This parameter is a *stochastic* function of the parameters feeding into it.\n",
    "* **Points** represent a delta-function PDF. This parameter is a *deterministic* function of the parameters feeding into it.\n",
    "* **Double circles** (or shading) indicate measured data. They are stochastic in the context of generating mock data, but fixed in the context of parameter inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q: What is this PGM telling us?\n",
    "\n",
    "<img src=\"../graphics/pgms_pixelcounts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q: How are these PGMs different, and what does the difference mean?\n",
    "\n",
    "<table><tr><td>\n",
    "<img src=\"../graphics/pgms_pixelcounts.png\">\n",
    "</td><td>\n",
    "<img src=\"../graphics/pgms_pixelcounts2.png\">\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Probabilistic Graphical Models\n",
    "\n",
    "* This procedure for simulating a mock X-ray image dataset can be usefully illustrated with a _directed acyclic graph_ called a \"Probabilistic Graphical Model\"\n",
    "\n",
    "* In the present context these provide something like a \"flow diagram\" showing how we might draw a sample mock dataset from our model\n",
    "\n",
    "* Let's look at the PGM for a simple X-ray image simulation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## One pixel, fixed inputs\n",
    "\n",
    "<img src=\"../graphics/pgms_one_pixel_input_fixed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## PGM pieces\n",
    "\n",
    "* Each _node_ in the graph represents a PDF, for the variable labeled inside it\n",
    "\n",
    "* Each _edge_ (arrow) in the graph represents a conditional dependence\n",
    "\n",
    "* Deterministic relationships are indicated by \"fixed\" variables represented by solid points\n",
    "\n",
    "> The fixed nodes are _also_ PDFs: fixing a parameter is the same as sampling it from a delta function PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## One pixel, sampled inputs\n",
    "\n",
    "<img src=\"../graphics/pgms_one_pixel_input_sampled.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## PGM interpretation\n",
    "\n",
    "* Following the arrows, the network of conditional dependences shows you how to go about simulating mock data ($N_k$)\n",
    "\n",
    "> For example: \n",
    "1. Draw a sample $\\theta$ vector;\n",
    "2. Compute $\\mu_k$ from it;\n",
    "3. Draw an $N_k$ given that $\\mu_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By mapping the conditional dependences of a model, PGMs illustrate how to factorize the sampling distribution:\n",
    "\n",
    "$P(\\theta) \\prod_k P(N_k|\\mu_k)P(\\mu_k|F_k,T)P(F_k|\\theta)$\n",
    "\n",
    "<img src=\"../graphics/pgms_pixelcounts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case, some PDFs are delta functions, so we can simplify:\n",
    "\n",
    "$P(\\theta) \\prod_k P(N_k|\\mu_k)P(\\mu_k|F_k,T)P(F_k|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$= \\underbrace{P(\\theta)} ~ \\underbrace{\\prod_k P\\left[N_k|\\mu_k(\\theta,T)\\right]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$= \\mathrm{prior}(\\theta) ~\\times~ (\\mathrm{sampling~distribution~of~}\\vec{N})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## PGM interpretation\n",
    "\n",
    "* The graph is also an illustration of a particular factorisation of a joint probability distribution: the PDF for every variable in the model \n",
    "\n",
    "* For example: \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(N_k|\\mu_k,H)\\;P(\\mu_k|\\theta_k,H)\\;P(\\theta_k|H) =  P(N_k,\\mu_k,\\theta_k|H)$\n",
    "\n",
    "> Note that the dependence of $N_k$ on $\\theta$ has been dropped here: $N_k$ is only dependent on $\\theta$ through $\\mu_k$, or if you prefer, $N_k$ is _conditionally independent_ of $\\theta$ given $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"../graphics/pgms_a-c-d.png\"></td>\n",
    "<td><img src=\"../graphics/pgms_c-y-d.png\"></td>\n",
    "</tr></table>\n",
    "\n",
    "* On your own, write down the probability expressions illustrated by these two graphs. \n",
    "* Then, discuss their meaning with your neighbor, and prepare to report back to the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Simulating the whole image\n",
    "\n",
    "* Our model for the cluster allows us to predict all of the image pixel values, separately: each of the $\\mu_k$ depends on $\\theta$ alone\n",
    "\n",
    "* Our simple Poisson model reflects an assumption about our detector, which is that the observed counts in the $k^{th}$ pixel are independent of the observed counts in all the other pixels. $N_k$ only depends on its pixel's expected counts $\\mu_k$\n",
    "\n",
    "* In this case, the sampling distribution for all the observed pixel values $\\boldsymbol{N}$ factorizes and simplifies to\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{N}|\\boldsymbol{\\mu},H) = \\prod_k P(N_k|\\mu_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## All pixels, fixed inputs\n",
    "\n",
    "<img src=\"../graphics/pgms_all_pixels_input_fixed.png\">\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{N},\\boldsymbol{\\mu},\\theta|H) = \\left[ \\prod_k P(N_k|\\mu_k,H)  P(\\mu_k|\\theta,H) \\right] P(\\theta|H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## PGM pieces\n",
    "\n",
    "* Each _node_ in the graph represents a PDF, for the variable labeled inside it\n",
    "\n",
    "* Each _edge_ (arrow) in the graph represents a conditional dependence\n",
    "\n",
    "* Deterministic relationships are indicated by \"fixed\" variables represented by solid points\n",
    "\n",
    "* Plates contain conditionally independent variables\n",
    "\n",
    "> Think of plates as illustrating a stack of layers, seen from above, that are only connected by the arrows coming from variables outside the plate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Take-home messages\n",
    "\n",
    "* Generating data is the key function of a statistical model\n",
    "\n",
    "* Noise-free mock datasets are central to inference; noisy mock datasets are vital for testing and model checking\n",
    "\n",
    "* Generating a mock dataset means drawing a sample from the joint probability distribution for all variables in the model\n",
    "\n",
    "* Probabilstic Graphical Models (PGMs) illustrate a particular factorization of this joint PDF, and can be viewed as \"flow charts\" for data simulation process\n",
    "\n",
    "* The sampling distribution captures the statistical uncertainties in the data; drawing from it \"adds noise\" to make the final mock dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Take-home messages**\n",
    "\n",
    "* Both simulation of mock data and model inference from data require a model for how the Universe (or our computer) generates data.\n",
    "* PGMs are a helpful way of visualizing the conditional dependences of a model (how the probability expressions factorize).\n",
    "\n",
    "Note: the `daft` Python package is useful for making pretty PGMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Coded Examples\n",
    "\n",
    "In the `examples` folder, there are some notebooks that show:\n",
    "\n",
    "* [A \"catalog\" dataset from the Sloan Digital Sky Survey](../examples/SDSScatalog/FirstLook.ipynb)\n",
    "\n",
    "* [An attempt to generate a mock dataset](../examples/SDSScatalog/GalaxySizes.ipynb) that (literally) looks like the real one\n",
    "\n",
    "* [How the `daft` python package can be used to draw PGMs](../examples/SDSScatalog/FirstPGM.ipynb)\n",
    "\n",
    "\n",
    "You might (one day) find some of this code useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Standard\" linear regression\n",
    "\n",
    "You're given a list of $x_k,y_k,\\sigma_k$ triplets, where $\\sigma_k$ is some estimate of the \"error\" on $y_k$. You'd like to fit a linear model, $y(x)=a+bx$. In the absence of any better information, assume that $\\vec{x}$ and $\\vec{\\sigma}$ are (somehow) known precisely, and that the \"error\" on $y_k$ is Gaussian (mean of $a+bx_k$ and standard deviation $\\sigma_k$).\n",
    "\n",
    "1. Draw the PGM and write down the corresponsing probability expressions for this problem.\n",
    "2. Simulate a few data sets, given some values (your choice) for the input parameters. The code below is a (crummy) starting point.\n",
    "3. What (unspecified) assumptions, if any, did you have to make? Which assumptions do you think are unlikely to hold in practice? Choose one (or more) of these assumptions and work out how to generalize the PGM/generative model to avoid making it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['xtick.labelsize'] = 'x-large'\n",
    "plt.rcParams['ytick.labelsize'] = 'x-large'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose some linear model parameters, somehow\n",
    "a = \n",
    "b = \n",
    "\n",
    "# choose some x and sigma values... somehow\n",
    "n = 10 # Number of data points. Feel free to change.\n",
    "x = np.array([\n",
    "sigma = np.array([\n",
    "\n",
    "# work out the values for any intermediate nodes\n",
    "    \n",
    "# generate the \"observed\" y values\n",
    "y = st.norm.rvs("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot x, y and sigma in the usual way\n",
    "plt.rcParams['figure.figsize'] = (12.0, 5.0)\n",
    "plt.errorbar(x, y, yerr=sigma, fmt='none');\n",
    "plt.xlabel('x', fontsize=14);\n",
    "plt.ylabel('y', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exoplanet transit photometry\n",
    "\n",
    "You've taken images of a particular field on several nights, in order to record the transit of an exoplanet in front of a star (resulting in a temporary decrease in its brightness). Some kind of model, parametrized by $\\theta$, describes the time series of the resulting flux. Before we get to measure a number of counts, however, each image is affected by night-specific variables such as weather. To account for these, you've also measured 10 other stars in the same field. The assumption is that the average intrinsic flux of these stars should be constant in time, so that they can be used to correct for nightly photometric variations, putting the multiple measurements of the target star on the same scale.\n",
    "\n",
    "Draw the PGM and write down the corresponsing probability expressions for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Thanks to Anja von der Linden for inspiring the above problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Galaxy cluster center offsets\n",
    "\n",
    "You've measured the centers of a sample of galaxy clusters in two ways: by choosing a brightest cluster galaxy (BCG) and by finding the centroid of each cluster's X-ray emission. The difference between the two should say something about the fidelity of the BCG selection method, among other things. The BCG positions are determined essentially perfectly, but the X-ray centroids come with a Gaussian statistical uncertainty of typically $\\sim30$ kpc (standard deviation) in both the $x$ and $y$ directions.\n",
    "\n",
    "The underlying model is assumed to be that the BCG and true X-ray centroid coincide perfectly in a fraction $f$ of clusters. In the remaining clusters, the true X-ray centroid and BCG are displaced according to a 2D Gaussian whose width in either direction is $\\sigma=100$ kpc.\n",
    "\n",
    "1. Draw the PGM and write down the corresponsing probability expressions for this problem.\n",
    "2. Simulate some data sets and visualize them, e.g. as a histogram of the offset distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Thanks to the bloody Universe for inspiring the above problem."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

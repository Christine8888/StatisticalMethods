{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximate Methods\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Appreciate the curse of dimensionality, and some of its consequences\n",
    "\n",
    "* See how the Laplace approximation can provide an approximate posterior PDF, in very convenient form\n",
    "\n",
    "* Gain a new appreciation for summary statistics, and see how ABC can provide an approximate posterior PDF despite the data likelihood never being computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data\n",
    "\n",
    "This term refers to any data set whose _volume_, _velocity_ or _variety_ is sufficiently high that we need to fundamentally change the way we approach it.\n",
    "\n",
    "\n",
    "> For example: SDSS gave astronomy a big data problem, in that we needed web-based SQL queries to allow us to make initial subsamples that we can work with. Before, we would have just downloaded the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Models\n",
    "\n",
    "Big datasets *ought* to support more interesting models of complex phenomena such as galaxy evolution, with many more parameters being able to be fitted.\n",
    "\n",
    "\n",
    "The problem is that posterior characterization gets more difficult - quickly - when the dimensionality of the model parameter space increases. \n",
    "\n",
    "\n",
    "> We saw this when we abandoned simple grids for MCMC samples - but sampling can only get us so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Hypervolumes\n",
    "\n",
    "Consider an $N$-dimensional hypersphere of radius $R$. Derive an expression for the fraction of its volume contained in a surface shell of thickness 1% of $R$. What is this volume fraction if $N=3$? How about $N=100$? $N=10,000$? \n",
    "\n",
    "> NB. $(1−x)^N \\approx e^{−Nx}$ when $Nx$ is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Vastness of High-dimensionality Space\n",
    "\n",
    "Consequent problems:\n",
    "\n",
    "* Uniform priors are a bad idea\n",
    "\n",
    "* Prior sampling is an even worse idea\n",
    "\n",
    "* Separation distances (and other misfit functions) become difficult to distinguish\n",
    "\n",
    "These (and related) problems are often known as _\"the curse of dimensionality\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 1: Don't Go There\n",
    "\n",
    "\"Dimensionality reduction\" is an excellent practical way to avoid _the curse_.\n",
    "\n",
    "\n",
    "* The dimensionality of the _data_ can be reduced by _compressing_ the data into a (much) smaller set of _\"summary statistics\"_\n",
    "\n",
    "\n",
    "* The resulting compressed dataset may be modelable with a much smaller number of parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Cosmic Shear\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"../graphics/approx_glshear_cfht.jpg\" width=70%></td>\n",
    "<td><img src=\"../graphics/approx_des_ngmix_xi.png\" width=70%></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Cosmic Shear\n",
    "\n",
    "* The \"data\" are automatically-measured galaxy shapes, whose tiny distortions are due to weak gravitational lensing by a complex framework of large scale dark matter structure pervading the entire universe between those background galaxies and us\n",
    "\n",
    "\n",
    "* Rather then hierarchically model this structure, with cosmological hyper-parameters, we compress the shape data into \"correlation function\" summary statistics _which can be directly predicted_ from a cosmological model\n",
    "\n",
    "\n",
    "* Issue: the exact form, and the multi-variate width, of the sampling distribution for the correlation function data is unknown (and dependent on cosmology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unknown Likelihood Function\n",
    "\n",
    "What should we do if we don't know (or don't trust our model for) the sampling distribution of our data?\n",
    "\n",
    "\n",
    "1. Model it in a flexible way (as in the [\"model-free models\"](modelfreemodels.ipynb) chunk)\n",
    "\n",
    "2. If we can generate mock data anyway - use ABC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approximate Bayesian Computation\n",
    "\n",
    "ABC is a family of sampling methods all based on the idea that if we can generate a mock dataset that is _close to_ he observed data, then the parameters of that model are _approximately_ a plausible draw from the posterior PDF\n",
    "\n",
    "> The closer we get, the better the approximation\n",
    "\n",
    "We are in this situation when the model is either too complex or perhaps \"too stochastic\" to enable a meaningful likelihood to be evaluated. \n",
    "\n",
    "> Example: cosmic shear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 2: Go Straight There\n",
    "\n",
    "Exploring high dimensionality parameter space is hard, unless you know where you are going\n",
    "\n",
    "\n",
    "* Linear models have log posterior surfaces that are quadratic in the parameter values: they are convex, and their peak can be found by (steepest/stochastic/conjugate/etc) gradient ascent \n",
    "\n",
    "\n",
    "* Models that are close to linear may be similarly numerically optimizable\n",
    "\n",
    "\n",
    "* Issuues: local maxima, log posterior evaluation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace Approximation\n",
    "\n",
    "Once the peak of the log posterior has been found by numerical optimization, we can approximate its shape with a Gaussian _even if the model is non-linear_:\n",
    "\n",
    "* Taylor expand around the peak position $\\hat{\\theta}$:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\log P(\\theta|d) \\approx \\log P(\\hat{\\theta}|d) - \\frac{1}{2} \\frac{\\partial^2 \\log P}{\\partial \\theta^2} \\bigg\\rvert_{\\theta=\\hat{\\theta}} (\\theta - \\hat{\\theta})^2 + O[(\\theta - \\hat{\\theta})^3]$\n",
    "\n",
    "* Ignore the higher order terms and exponentiate:\n",
    "\n",
    "$\\;\\;\\;\\;\\;P(\\theta|d) \\approx P(\\hat{\\theta}|d) \\exp \\left[ -\\frac{1}{2} (\\theta - \\hat{\\theta})^T H (\\theta - \\hat{\\theta}) \\right]$\n",
    "\n",
    "where $H$ is the \"Hessian\" matrix of second derivatives, $H_{ij} = \\frac{\\partial^2 \\log P}{\\partial \\theta_i \\partial \\theta_j} \\bigg\\rvert_{\\theta=\\hat{\\theta}}$. This is proportional to a multivariate Gaussian, with covariance matrix $C = H^{-1}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical Laplace Approximation\n",
    "\n",
    "* Ask your optimizer to return the `inv_hessian` and assign thatas your covariance matrix.\n",
    "\n",
    "\n",
    "* Note that the Laplace approximation is normalized: you can use it to compute approximate Evidences\n",
    "\n",
    "\n",
    "* The Laplace approximation may not be very accurate: but it often provides a good starting point a sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Good luck out there!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
